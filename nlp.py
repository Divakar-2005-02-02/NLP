# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19_AqQGuFJS4mwaPzBtPdLl6h8DmkkRs4
"""

pip install pandas numpy scikit-learn transformers torch

# Install required packages
!pip install pandas numpy scikit-learn transformers torch tensorflow

import re
import string
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

## Data Loading and Preprocessing

# Read the dataset
with open('train.txt', 'r') as file:
    lines = file.readlines()

code_snippets = []
comments = []

i = 0
while i < len(lines):
    line = lines[i].strip()

    # Skip Snippet Labels like '# Snippet 1'
    if line.startswith('# Snippet'):
        i += 1
        continue

    # If the line is a function definition
    if line.startswith('def') or line.startswith('class'):
        code = line
        i += 1
        # Gather full code snippet
        while i < len(lines) and lines[i].strip() != '' and not lines[i].strip().startswith('def') and not lines[i].strip().startswith('class') and not lines[i].strip().startswith('# Snippet'):
            code += '\n' + lines[i].rstrip()
            i += 1

        # Extract the comment inside the code
        match = re.search(r'#\s*(.*)', code)
        if match:
            comment = match.group(1).strip()
            comments.append(comment)
            code_snippets.append(code)
    else:
        i += 1

# Result check
print(f"Extracted {len(code_snippets)} code snippets and {len(comments)} comments.")
print("\nSample Code Snippet:\n", code_snippets[0])
print("\nSample Comment:\n", comments[0])

## Text Cleaning

def clean_comment(comment):
    comment = comment.lower()
    comment = re.sub(rf"[{re.escape(string.punctuation)}]", "", comment)
    return comment

def clean_code(code):
    code = code.lower()
    code = re.sub(r'\s+', ' ', code).strip()
    return code

# Clean all data
cleaned_comments = [clean_comment(c) for c in comments]
cleaned_codes = [clean_code(c) for c in code_snippets]

## Tokenization and Sequence Preparation

# Initialize tokenizers with special tokens
comment_tokenizer = Tokenizer(oov_token='<OOV>', filters='')
comment_tokenizer.fit_on_texts(['<start>', '<end>'] + cleaned_comments)

code_tokenizer = Tokenizer(oov_token='<OOV>', filters='')
code_tokenizer.fit_on_texts(cleaned_codes)

# Add start/end tokens to comments
cleaned_comments = ['<start> ' + c + ' <end>' for c in cleaned_comments]

# Convert texts to sequences
comment_sequences = comment_tokenizer.texts_to_sequences(cleaned_comments)
code_sequences = code_tokenizer.texts_to_sequences(cleaned_codes)

# Pad sequences
max_comment_len = max(len(seq) for seq in comment_sequences)
max_code_len = max(len(seq) for seq in code_sequences)

comment_padded = pad_sequences(comment_sequences, maxlen=max_comment_len, padding='post')
code_padded = pad_sequences(code_sequences, maxlen=max_code_len, padding='post')

# Decoder target data (shifted by one timestep)
decoder_target_data = np.zeros_like(comment_padded)
decoder_target_data[:, :-1] = comment_padded[:, 1:]

## Model Architecture

# Vocabulary sizes
code_vocab_size = len(code_tokenizer.word_index) + 1
comment_vocab_size = len(comment_tokenizer.word_index) + 1

# Hyperparameters
embedding_dim = 128
lstm_units = 256

# Encoder
encoder_inputs = Input(shape=(max_code_len,))
enc_emb = Embedding(input_dim=code_vocab_size, output_dim=embedding_dim, mask_zero=True)(encoder_inputs)
encoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True)(enc_emb)

# Decoder
decoder_inputs = Input(shape=(max_comment_len,))
dec_emb_layer = Embedding(input_dim=comment_vocab_size, output_dim=embedding_dim, mask_zero=True)
dec_emb = dec_emb_layer(decoder_inputs)

decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])

decoder_dense = Dense(comment_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

## Training

history = model.fit(
    [code_padded, comment_padded],
    decoder_target_data[..., np.newaxis],  # add channel dimension
    batch_size=16,
    epochs=30,
    validation_split=0.1
)

## Inference Setup

# Encoder inference model
encoder_model_inf = Model(encoder_inputs, [state_h, state_c])

# Decoder inference setup
decoder_state_input_h = Input(shape=(lstm_units,))
decoder_state_input_c = Input(shape=(lstm_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_inputs_single = Input(shape=(1,))
decoder_embedding_single = dec_emb_layer(decoder_inputs_single)

decoder_outputs_single, state_h_single, state_c_single = decoder_lstm(
    decoder_embedding_single, initial_state=decoder_states_inputs)
decoder_states = [state_h_single, state_c_single]
decoder_outputs_single = decoder_dense(decoder_outputs_single)

decoder_model_inf = Model(
    [decoder_inputs_single] + decoder_states_inputs,
    [decoder_outputs_single] + decoder_states
)

## Decoding Function

def decode_sequence(input_seq):
    # Get encoder states
    states_value = encoder_model_inf.predict(input_seq)

    # Generate empty target sequence with just the start token
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = comment_tokenizer.word_index['<start>']

    stop_condition = False
    decoded_sentence = []

    while not stop_condition:
        output_tokens, h, c = decoder_model_inf.predict(
            [target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = comment_tokenizer.index_word.get(sampled_token_index, '')
        decoded_sentence.append(sampled_word)

        # Exit condition: hit max length or find stop token
        if (sampled_word == '<end>' or
            len(decoded_sentence) > max_comment_len):
            stop_condition = True

        # Update the target sequence and states
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]

    return ' '.join(decoded_sentence[:-1])  # Exclude <end> token

## Testing the Model

# Test with a sample
sample_idx = 0
sample_input = code_padded[sample_idx:sample_idx+1]

predicted_comment = decode_sequence(sample_input)
actual_comment = comments[sample_idx]

print("\nInput Code:")
print(code_snippets[sample_idx])
print("\nActual Comment:", actual_comment)
print("Predicted Comment:", predicted_comment)

# Install required packages
!pip install --upgrade transformers datasets

import re
import string
import numpy as np
import pandas as pd
from datasets import Dataset
from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,
                          Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq)

## Data Loading and Preprocessing
# Read the dataset
with open('train.txt', 'r') as file:
    lines = file.readlines()

code_snippets = []
comments = []

i = 0
while i < len(lines):
    line = lines[i].strip()

    if line.startswith('# Snippet'):
        i += 1
        continue

    if line.startswith('def') or line.startswith('class'):
        code = line
        i += 1
        while i < len(lines) and lines[i].strip() != '' and not lines[i].strip().startswith('def') \
              and not lines[i].strip().startswith('class') and not lines[i].strip().startswith('# Snippet'):
            code += '\n' + lines[i].rstrip()
            i += 1

        match = re.search(r'#\s*(.*)', code)
        if match:
            comment = match.group(1).strip()
            comments.append(comment)
            code_snippets.append(code)
    else:
        i += 1

print(f"Extracted {len(code_snippets)} code snippets and {len(comments)} comments.")
print("\nSample Code Snippet:\n", code_snippets[0])
print("\nSample Comment:\n", comments[0])

## Text Cleaning

def clean_comment(comment):
    comment = comment.lower()
    comment = re.sub(rf"[{re.escape(string.punctuation)}]", "", comment)
    return comment

def clean_code(code):
    code = code.lower()
    code = re.sub(r'\s+', ' ', code).strip()
    return code

cleaned_comments = ["<s> " + clean_comment(c) + " </s>" for c in comments]
cleaned_codes = [clean_code(c) for c in code_snippets]

## Create Dataset
data = pd.DataFrame({"code": cleaned_codes, "comment": cleaned_comments})
dataset = Dataset.from_pandas(data)

## Tokenizer and Preprocessing
model_checkpoint = "Salesforce/codet5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

max_source_length = 256
max_target_length = 128

def preprocess_function(examples):
    inputs = examples["code"]
    targets = examples["comment"]
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding="max_length", truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, padding="max_length", truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

processed_dataset = dataset.map(preprocess_function, batched=True)

## Load Model
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

## Training Arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    num_train_epochs=5,
    save_strategy="no",
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=10,
    report_to="none"  # Disable WandB or other integrations
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

## Train the model
trainer.train()

## Inference Function
def generate_comment(code):
    code = clean_code(code)
    inputs = tokenizer(code, return_tensors="pt", padding=True, truncation=True, max_length=max_source_length)
    output = model.generate(**inputs, max_length=max_target_length)
    return tokenizer.decode(output[0], skip_special_tokens=True)

## Test Example
print("\nInput Code:")
print(code_snippets[0])
print("\nActual Comment:", comments[0])
print("Predicted Comment:", generate_comment(code_snippets[0]))

